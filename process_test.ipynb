{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Construct VectorDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resource module not available on Windows\n",
      "< VectorDatabase initialized > \n",
      "  - loading data into ChromaDB \n",
      "     - loading [faq] ...\n",
      "        ... collection [faq] deleted.\n",
      "     - loading [insurance] ...\n",
      "        ... collection [insurance] deleted.\n",
      "     - loading [finance] ...\n",
      "        ... collection [finance] deleted.\n"
     ]
    }
   ],
   "source": [
    "from Utils import *\n",
    "import os\n",
    "\n",
    "# initialize the database\n",
    "db = VectorDatabase()\n",
    "\n",
    "# gpu if available\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:4000\"\n",
    "\n",
    "# initialize the database\n",
    "db.initialize_process(chunk_size=256 ,chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< VectorDatabase initialized > \n",
      "< Retriever initialized > \n",
      "  - Answers saved to output.json \n",
      "< Evaluation by Ground Truths > \n",
      "  - Retrieval accuracy: 87.33%\n",
      "     - Category: [insurance], Accuracy: 90.00%\n",
      "     - Category: [finance], Accuracy: 74.00%\n",
      "     - Category: [faq], Accuracy: 98.00%\n"
     ]
    }
   ],
   "source": [
    "from Utils import *\n",
    "\n",
    "# initialize the retriever\n",
    "retriever = Retriever()\n",
    "# do question \n",
    "retriever.process_questions(method='Vector')\n",
    "\n",
    "# evaluate the accuracy\n",
    "evaluator = Evaluation()\n",
    "evaluator.output_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25 Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< VectorDatabase initialized > \n",
      "< Retriever initialized > \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\marks\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.592 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Answers saved to output.json \n",
      "< Evaluation by Ground Truths > \n",
      "  - Retrieval accuracy: 66.67%\n",
      "     - Category: [insurance], Accuracy: 70.00%\n",
      "     - Category: [finance], Accuracy: 44.00%\n",
      "     - Category: [faq], Accuracy: 86.00%\n"
     ]
    }
   ],
   "source": [
    "from Utils import *\n",
    "\n",
    "# initialize the retriever\n",
    "retriever = Retriever(question_path='enhanced_questions.json')\n",
    "# do question \n",
    "retriever.process_questions(method='original')\n",
    "\n",
    "# evaluate the accuracy\n",
    "evaluator = Evaluation()\n",
    "evaluator.output_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25 + Vector Fusion Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< VectorDatabase initialized > \n",
      "< Retriever initialized > \n"
     ]
    }
   ],
   "source": [
    "from Utils import *\n",
    "\n",
    "# initialize the retriever\n",
    "retriever = Retriever(question_path='enhanced_questions.json')\n",
    "# do question \n",
    "retriever.process_questions(method='BM25_Vector')\n",
    "\n",
    "# evaluate the accuracy\n",
    "evaluator = Evaluation()\n",
    "evaluator.output_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama index BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< VectorDatabase initialized > \n",
      "< Retriever initialized > \n",
      "  - Answers saved to output.json \n",
      "< Evaluation by Ground Truths > \n",
      "  - Retrieval accuracy: 80.67%\n",
      "     - Category: [insurance], Accuracy: 96.00%\n",
      "     - Category: [finance], Accuracy: 56.00%\n",
      "     - Category: [faq], Accuracy: 90.00%\n"
     ]
    }
   ],
   "source": [
    "from Utils import *\n",
    "\n",
    "# initialize the retriever\n",
    "retriever = Retriever(question_path='enhanced_questions.json')\n",
    "# do question \n",
    "retriever.process_questions(method='BM25')\n",
    "\n",
    "# evaluate the accuracy\n",
    "evaluator = Evaluation()\n",
    "evaluator.output_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama index BM25 filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import *\n",
    "\n",
    "# initialize the retriever\n",
    "retriever = Retriever(question_path='enhanced_questions.json')\n",
    "# do question \n",
    "retriever.process_questions(method='Vector_filter_BM25')\n",
    "\n",
    "# evaluate the accuracy\n",
    "evaluator = Evaluation()\n",
    "evaluator.output_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resource module not available on Windows\n",
      "< VectorDatabase initialized > \n",
      "< Retriever initialized > \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\marks\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.616 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Answers saved to output.json \n",
      "< Evaluation by Ground Truths > \n",
      "  - Retrieval accuracy: 86.00%\n",
      "     - Category: [insurance], Accuracy: 94.00%\n",
      "     - Category: [finance], Accuracy: 66.00%\n",
      "     - Category: [faq], Accuracy: 98.00%\n"
     ]
    }
   ],
   "source": [
    "from Utils import *\n",
    "\n",
    "# initialize the retriever\n",
    "retriever = Retriever(question_path='enhanced_questions.json')\n",
    "# do question \n",
    "retriever.process_questions(method='w')\n",
    "\n",
    "# evaluate the accuracy\n",
    "evaluator = Evaluation()\n",
    "evaluator.output_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import *\n",
    "import os\n",
    "\n",
    "# initialize the database\n",
    "db = VectorDatabase()\n",
    "\n",
    "# initialize the database\n",
    "s = db.summary_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marks\\anaconda3\\envs\\poetry3.11\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_id\" in DeployedModel has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\marks\\anaconda3\\envs\\poetry3.11\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\marks\\anaconda3\\envs\\poetry3.11\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_kwargs\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\marks\\anaconda3\\envs\\poetry3.11\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPI has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\marks\\anaconda3\\envs\\poetry3.11\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in TextGenerationInference has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00166ce5ef947db937efec6047e0012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "system_prompt = \"你是一個來自台灣的AI助理，你的名字是 TAIDE，樂於以台灣人的立場幫助使用者，會用正體中文回答問題。\"\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "\"taide/Llama3-TAIDE-LX-8B-Chat-Alpha1\"\n",
    ")\n",
    "stopping_ids = [\n",
    "tokenizer.eos_token_id,\n",
    "tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]\n",
    "\n",
    "# # Transform a string into input zephyr-specific input\n",
    "# def completion_to_prompt(completion):\n",
    "#     return f\"<|system|>\\n</s>\\n<|user|>\\n{completion}</s>\\n<|assistant|>\\n\"\n",
    "\n",
    "\n",
    "# # Transform a list of chat messages into zephyr-specific input\n",
    "# def messages_to_prompt(messages):\n",
    "#     prompt = \"\"\n",
    "#     for message in messages:\n",
    "#         if message.role == \"system\":\n",
    "#             prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
    "#         elif message.role == \"user\":\n",
    "#             prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
    "#         elif message.role == \"assistant\":\n",
    "#             prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
    "\n",
    "#     # ensure we start with a system prompt, insert blank if needed\n",
    "#     if not prompt.startswith(\"<|system|>\\n\"):\n",
    "#         prompt = \"<|system|>\\n</s>\\n\" + prompt\n",
    "\n",
    "#     # add final assistant prompt\n",
    "#     prompt = prompt + \"<|assistant|>\\n\"\n",
    "\n",
    "#     return prompt\n",
    "\n",
    "\n",
    "import torch\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    model_name=\"taide/Llama3-TAIDE-LX-8B-Chat-Alpha1\",\n",
    "    tokenizer_name=\"taide/Llama3-TAIDE-LX-8B-Chat-Alpha1\",\n",
    "    context_window=2048,\n",
    "    max_new_tokens=400,\n",
    "    generate_kwargs={\"temperature\": 0.6, \"top_p\": 0.9, 'do_sample': True},\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    stopping_ids=stopping_ids,\n",
    "    tokenizer_kwargs={\"max_length\": 8000},\n",
    "    # messages_to_prompt=messages_to_prompt,\n",
    "    # completion_to_prompt=completion_to_prompt,\n",
    "    device_map=\"cuda\",\n",
    "    model_kwargs={\"torch_dtype\": torch.float16},\n",
    ")\n",
    "\n",
    "import logging\n",
    "\n",
    "# 定義 logging 輸出格式\n",
    "FORMAT = '%(asctime)s %(filename)s %(levelname)s:%(message)s'\n",
    "logging.basicConfig(level=logging.INFO, format=FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_gen_str = \"\"\"\\\n",
    "你是一個來自台灣非常強大的中文問答助理，你很擅長根據一個原始問題進行改寫，生成出多個相關\\\n",
    "、幫助加強語意的問題。 請生成 {num_queries} 搜尋問題，一個問題一行，以加強語意的角度，\\\n",
    "改寫下列輸入的問題：\n",
    "問題： {query}\n",
    "生成的問題：\n",
    "\"\"\"\n",
    "query_gen_prompt = PromptTemplate(query_gen_str)\n",
    "\n",
    "def generate_queries(query: str, llm, num_queries: int = 4):\n",
    "    response = llm.predict(\n",
    "        query_wrapper_prompt, num_queries=num_queries, query=query\n",
    "    )\n",
    "    # assume LLM proper put each query on a newline\n",
    "    queries = response.split(\"\\n\")\n",
    "    queries_str = \"\\n\".join(queries)\n",
    "    print(f\"Generated queries:\\n{queries_str}\")\n",
    "    return queries\n",
    "\n",
    "generate_queries(\"在2022年第3季，長榮公司有無從事衍生工具交易的情事？\", Settings.llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d62868981314821b5c78667df4d5f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始問題: 在2022年第3季，長榮公司有無從事衍生工具交易的情事？\n",
      "\n",
      "改寫後的問題:\n",
      "1. 請依照以下規則，將問題改寫成3個不同版本：\n",
      "2. 1. 保持原始問題的核心意思\n",
      "3. 2. 使用不同的表達方式\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class QuestionOptimizer:\n",
    "    def __init__(self, model_name: str = \"Llama3-TAIDE-LX-8B-Chat-Alpha1\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        \n",
    "    def _generate_response(self, prompt: str) -> str:\n",
    "        \"\"\"使用模型生成回應\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,  # 明確啟用截斷\n",
    "            max_length=2048,  # 設定輸入長度限制\n",
    "            padding=True\n",
    "        ).to(self.model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=400,  # 只使用 max_new_tokens\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9\n",
    "            )\n",
    "            \n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def enhance_question(self, query: str, num_variants: int = 3) -> List[str]:\n",
    "        \"\"\"生成改寫後的問題\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "請依照以下規則，將問題改寫成{num_variants}個不同版本：\n",
    "1. 保持原始問題的核心意思\n",
    "2. 使用不同的表達方式\n",
    "3. 添加相關的關鍵詞\n",
    "4. 重組句子結構\n",
    "5. 確保改寫後的問題更容易被搜尋系統理解\n",
    "\n",
    "原始問題：{query}\n",
    "\"\"\"\n",
    "        response = self._generate_response(prompt)\n",
    "        enhanced_questions = [q.strip() for q in response.split('\\n') if q.strip()]\n",
    "        return enhanced_questions[:num_variants]\n",
    "\n",
    "    def process_batch(self, questions: List[dict]) -> List[dict]:\n",
    "        \"\"\"批次處理問題\"\"\"\n",
    "        return [{\n",
    "            'qid': q['qid'],\n",
    "            'original': q['query'],\n",
    "            'enhanced': self.enhance_question(q['query'])\n",
    "        } for q in questions]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    optimizer = QuestionOptimizer()\n",
    "    test_query = \"在2022年第3季，長榮公司有無從事衍生工具交易的情事？\"\n",
    "    enhanced = optimizer.enhance_question(test_query)\n",
    "    \n",
    "    print(\"原始問題:\", test_query)\n",
    "    print(\"\\n改寫後的問題:\")\n",
    "    for i, q in enumerate(enhanced, 1):\n",
    "        print(f\"{i}. {q}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
