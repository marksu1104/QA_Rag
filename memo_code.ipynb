{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class QuestionOptimizer:\n",
    "    def __init__(self, model_name: str = \"Llama3-TAIDE-LX-8B-Chat-Alpha1\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        \n",
    "    def _generate_response(self, prompt: str) -> str:\n",
    "        \"\"\"使用模型生成回應\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,  # 明確啟用截斷\n",
    "            max_length=2048,  # 設定輸入長度限制\n",
    "            padding=True\n",
    "        ).to(self.model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=400,  # 只使用 max_new_tokens\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9\n",
    "            )\n",
    "            \n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def enhance_question(self, query: str, num_variants: int = 3) -> List[str]:\n",
    "        \"\"\"生成改寫後的問題\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "請依照以下規則，將問題改寫成{num_variants}個不同版本：\n",
    "1. 保持原始問題的核心意思\n",
    "2. 使用不同的表達方式\n",
    "3. 添加相關的關鍵詞\n",
    "4. 重組句子結構\n",
    "5. 確保改寫後的問題更容易被搜尋系統理解\n",
    "\n",
    "原始問題：{query}\n",
    "\"\"\"\n",
    "        response = self._generate_response(prompt)\n",
    "        enhanced_questions = [q.strip() for q in response.split('\\n') if q.strip()]\n",
    "        return enhanced_questions[:num_variants]\n",
    "\n",
    "    def process_batch(self, questions: List[dict]) -> List[dict]:\n",
    "        \"\"\"批次處理問題\"\"\"\n",
    "        return [{\n",
    "            'qid': q['qid'],\n",
    "            'original': q['query'],\n",
    "            'enhanced': self.enhance_question(q['query'])\n",
    "        } for q in questions]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    optimizer = QuestionOptimizer()\n",
    "    test_query = \"在2022年第3季，長榮公司有無從事衍生工具交易的情事？\"\n",
    "    enhanced = optimizer.enhance_question(test_query)\n",
    "    \n",
    "    print(\"原始問題:\", test_query)\n",
    "    print(\"\\n改寫後的問題:\")\n",
    "    for i, q in enumerate(enhanced, 1):\n",
    "        print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import *\n",
    "from llama_index.core.node_parser import SentenceSplitter,SentenceWindowNodeParser\n",
    "from llama_index.core import VectorStoreIndex, Document, StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# initialize the database\n",
    "db = VectorDatabase()\n",
    "dict = db.corpus_dict_finance\n",
    "\n",
    "my_embedding = HuggingFaceEmbedding(\n",
    "    model_name=\"TencentBAC/Conan-embedding-v1\"\n",
    ")\n",
    "# Prepare documents\n",
    "documents = [\n",
    "    Document(\n",
    "        text=text,\n",
    "        id_=f\"doc_id_{id}\",\n",
    "        metadata={\"category\": 'finance', \"pid\": id}\n",
    "    ) for id, text in dict.items()\n",
    "]\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=256, chunk_overlap=200)\n",
    "#\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")\n",
    "nodes_sentence = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "sentence_vector_index = VectorStoreIndex(\n",
    "    nodes=nodes_sentence,\n",
    "    embed_model=my_embedding,\n",
    "    show_progress=True,\n",
    ")\n",
    "sentence_vector_index.storage_context.persist(persist_dir='./database/sentence/fiance.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import *\n",
    "from llama_index.core.node_parser import SentenceSplitter,SentenceWindowNodeParser\n",
    "from llama_index.core import VectorStoreIndex, Document, StorageContext, load_index_from_storage\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# initialize the database\n",
    "db = VectorDatabase()\n",
    "dict = db.corpus_dict_finance\n",
    "\n",
    "my_embedding = HuggingFaceEmbedding(\n",
    "    model_name=\"TencentBAC/Conan-embedding-v1\"\n",
    ")\n",
    "# Prepare documents\n",
    "documents = [\n",
    "    Document(\n",
    "        text=text,\n",
    "        id_=f\"doc_id_{id}\",\n",
    "        metadata={\"category\": 'finance', \"pid\": id}\n",
    "    ) for id, text in dict.items()\n",
    "]\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=256, chunk_overlap=200)\n",
    "#\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")\n",
    "nodes_sentence = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "sentence_vector_index = VectorStoreIndex(\n",
    "    nodes=nodes_sentence,\n",
    "    embed_model=my_embedding,\n",
    "    show_progress=True,\n",
    ")\n",
    "sentence_vector_index.storage_context.persist(persist_dir='./database/sentence/fiance')\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, Document, StorageContext, load_index_from_storage\n",
    "index = load_index_from_storage(\n",
    "    StorageContext.from_defaults(persist_dir=\"./database/sentence/fiance\"),\n",
    "    embed_model=my_embedding,\n",
    ")\n",
    "\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.vector_stores import MetadataFilter, MetadataFilters, FilterOperator\n",
    "# Define metadata filters\n",
    "filters = MetadataFilters(\n",
    "    filters=[\n",
    "        MetadataFilter(key=\"category\", value=\"finance\", operator=FilterOperator.EQ),\n",
    "        MetadataFilter(key=\"pid\", value=[41, 70, 359, 870, 900, 951, 59], operator=FilterOperator.IN)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize the vector index retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=1,  # Retrieve all possible results\n",
    "    filters=filters\n",
    ")\n",
    "results = retriever.retrieve(\"鴻海在2023年第1季度中，集團存貨之帳面金額是多少？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.document_summary import DocumentSummaryIndexLLMRetriever\n",
    "\n",
    "from typing import Any, Callable, Dict, List, Optional, Union, cast\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from llama_index.core.llms.callbacks import llm_completion_callback\n",
    "from llama_index.core.llms import (\n",
    "    CustomLLM,\n",
    "    CompletionResponse,\n",
    "    CompletionResponseGen,\n",
    "    LLMMetadata,\n",
    ")\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model_name = \"Llama3-TAIDE-LX-8B-Chat-Alpha1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, low_cpu_mem_usage=True, device_map=\"cuda\", torch_dtype=torch.bfloat16).eval()\n",
    "#自定义本地模型\n",
    "class OurLLM(CustomLLM):\n",
    "    context_window: int = 4096\n",
    "    num_output: int = 1024\n",
    "    model_name_: str = \"custom\"\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        \"\"\"Get LLM metadata.\"\"\"\n",
    "        return LLMMetadata(\n",
    "            context_window=self.context_window,\n",
    "            num_output=self.num_output,\n",
    "            model_name_=self.model_name,\n",
    "        )\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
    "        text, history = model.chat(tokenizer, prompt, history=[], temperature=0.1)\n",
    "        return CompletionResponse(text=text)\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(\n",
    "            self, prompt: str, **kwargs: Any\n",
    "    ) -> CompletionResponseGen:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "system_prompt = \"你是一個來自台灣的AI助理，你的名字是 TAIDE，樂於以台灣人的立場幫助使用者，會用正體中文回答問題。\"\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "\"taide/Llama3-TAIDE-LX-8B-Chat-Alpha1\"\n",
    ")\n",
    "stopping_ids = [\n",
    "tokenizer.eos_token_id,\n",
    "tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]\n",
    "\n",
    "# # Transform a string into input zephyr-specific input\n",
    "# def completion_to_prompt(completion):\n",
    "#     return f\"<|system|>\\n</s>\\n<|user|>\\n{completion}</s>\\n<|assistant|>\\n\"\n",
    "\n",
    "\n",
    "# # Transform a list of chat messages into zephyr-specific input\n",
    "# def messages_to_prompt(messages):\n",
    "#     prompt = \"\"\n",
    "#     for message in messages:\n",
    "#         if message.role == \"system\":\n",
    "#             prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
    "#         elif message.role == \"user\":\n",
    "#             prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
    "#         elif message.role == \"assistant\":\n",
    "#             prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
    "\n",
    "#     # ensure we start with a system prompt, insert blank if needed\n",
    "#     if not prompt.startswith(\"<|system|>\\n\"):\n",
    "#         prompt = \"<|system|>\\n</s>\\n\" + prompt\n",
    "\n",
    "#     # add final assistant prompt\n",
    "#     prompt = prompt + \"<|assistant|>\\n\"\n",
    "\n",
    "#     return prompt\n",
    "\n",
    "\n",
    "import torch\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    model_name=\"taide/Llama3-TAIDE-LX-8B-Chat-Alpha1\",\n",
    "    tokenizer_name=\"taide/Llama3-TAIDE-LX-8B-Chat-Alpha1\",\n",
    "    context_window=2048,\n",
    "    max_new_tokens=400,\n",
    "    generate_kwargs={\"temperature\": 0.6, \"top_p\": 0.9, 'do_sample': True},\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    stopping_ids=stopping_ids,\n",
    "    tokenizer_kwargs={\"max_length\": 8000},\n",
    "    # messages_to_prompt=messages_to_prompt,\n",
    "    # completion_to_prompt=completion_to_prompt,\n",
    "    device_map=\"cuda\",\n",
    "    model_kwargs={\"torch_dtype\": torch.float16},\n",
    ")\n",
    "\n",
    "import logging\n",
    "\n",
    "# 定義 logging 輸出格式\n",
    "FORMAT = '%(asctime)s %(filename)s %(levelname)s:%(message)s'\n",
    "logging.basicConfig(level=logging.INFO, format=FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Any, Optional\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    " \n",
    "# Load \n",
    "path = \"reference/finance/\"\n",
    "file = \"351.pdf\"\n",
    "# Get elements\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=path+file,\n",
    "    extract_images_in_pdf=True,\n",
    "    infer_table_structure=True, \n",
    "    # Post processing to aggregate text once we have the title \n",
    "    chunking_strategy=\"by_title\",\n",
    "    # Chunking params to aggregate text blocks\n",
    "    # Require maximum chunk size of 4000 chars\n",
    "    # Attempt to create a new chunk at 3800 chars\n",
    "    # Attempt to keep chunks > 2000 chars \n",
    "    max_characters=4000, \n",
    "    new_after_n_chars=3800, \n",
    "    combine_text_under_n_chars=2000,\n",
    "    image_output_dir_path=path\n",
    ")\n",
    "\n",
    "class Element(BaseModel):\n",
    "    type: str\n",
    "    text: Any\n",
    " \n",
    "# Categorize by type\n",
    "categorized_elements = []\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
    "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
    " \n",
    "# Tables\n",
    "table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
    "print(table_elements) \n",
    "# output: 28 elements in the PDF file\n",
    " \n",
    "# Text\n",
    "text_elements = [e for e in categorized_elements if e.type == \"text\"]\n",
    "print(text_elements) \n",
    "# output: 127 elements in the PDF file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
