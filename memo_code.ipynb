{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckip_transformers import __version__\n",
    "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
    "\n",
    "\n",
    "# Show version\n",
    "print(__version__)\n",
    "\n",
    "# Initialize drivers\n",
    "print(\"Initializing drivers ... WS\")\n",
    "ws_driver = CkipWordSegmenter(model=\"albert-base\", device=0)\n",
    "print(\"Initializing drivers ... POS\")\n",
    "pos_driver = CkipPosTagger(model=\"albert-base\", device=0)\n",
    "print(\"Initializing drivers ... NER\")\n",
    "ner_driver = CkipNerChunker(model=\"albert-base\", device=0)\n",
    "print(\"Initializing drivers ... all done\")\n",
    "print()\n",
    "\n",
    "def clean(sentence_ws, sentence_pos):\n",
    "  short_with_pos = []\n",
    "  short_sentence = []\n",
    "  stop_pos = set(['Nep', 'Nh', 'Nb']) # 這 3 種詞性不保留\n",
    "  for word_ws, word_pos in zip(sentence_ws, sentence_pos):\n",
    "    # 只留名詞和動詞\n",
    "    print(word_pos)\n",
    "    is_N_or_V = word_pos.startswith(\"V\") or word_pos.startswith(\"N\")\n",
    "    # 去掉名詞裡的某些詞性\n",
    "    is_not_stop_pos = word_pos not in stop_pos\n",
    "    # 只剩一個字的詞也不留\n",
    "    is_not_one_charactor = not (len(word_ws) == 1)\n",
    "    # 組成串列\n",
    "    if is_N_or_V and is_not_stop_pos and is_not_one_charactor:\n",
    "      short_with_pos.append(f\"{word_ws}({word_pos})\")\n",
    "      short_sentence.append(f\"{word_ws}\")\n",
    "  return (\" \".join(short_sentence), \" \".join(short_with_pos))\n",
    "\n",
    "def main():\n",
    "    text = [\n",
    "        \"\"\"用。\"\"\"\n",
    "    ]\n",
    "    ws = ws_driver(text)\n",
    "    pos = pos_driver(ws)\n",
    "    ner = ner_driver(text)\n",
    "    print(pos)\n",
    "    print()\n",
    "    print('=====')\n",
    "    for sentence, sentence_ws, sentence_pos, sentence_ner in zip(text, ws, pos, ner):\n",
    "        print(\"原文：\")\n",
    "        print(sentence)\n",
    "        (short, res) = clean(sentence_ws, sentence_pos)\n",
    "        print(\"斷詞後：\")\n",
    "        print(short)\n",
    "        print(\"斷詞後+詞性標注：\")\n",
    "        print(res)\n",
    "        print('=====')\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing PaddleOCR...\n",
      "PaddleOCR initialized successfully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "from typing import Optional, Dict, List, Tuple, Union\n",
    "import fitz \n",
    "from paddleocr import PaddleOCR\n",
    "import pdfplumber\n",
    "import re\n",
    "import string\n",
    "try:\n",
    "    # Initialize PaddleOCR with error handling\n",
    "    print(\"Initializing PaddleOCR...\")\n",
    "    ocr = PaddleOCR(use_angle_cls=True, lang='chinese_cht', use_gpu=True)\n",
    "    print(\"PaddleOCR initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing PaddleOCR: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import cv2\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "def process_image(image_bytes):\n",
    "    \"\"\"處理 PDF 中的圖片進行 OCR\"\"\"\n",
    "    if not image_bytes:\n",
    "        return {\"ocr_text\": \"\"}\n",
    "        \n",
    "    try:\n",
    "        # 創建臨時文件\n",
    "        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp_file:\n",
    "            tmp_path = Path(tmp_file.name)\n",
    "            \n",
    "            # 將圖片字節轉換為 numpy 數組\n",
    "            nparr = np.frombuffer(image_bytes, np.uint8)\n",
    "            img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "            \n",
    "            if img is None:\n",
    "                logging.error(\"無法解碼圖片數據\")\n",
    "                return {\"ocr_text\": \"\"}\n",
    "            \n",
    "            # 圖片預處理\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "            denoised = cv2.fastNlMeansDenoising(binary)\n",
    "            \n",
    "            # 保存處理後的圖片\n",
    "            cv2.imwrite(str(tmp_path), denoised)\n",
    "            \n",
    "            # OCR 處理\n",
    "            result = ocr.ocr(str(tmp_path), cls=True)\n",
    "            \n",
    "            if not result:\n",
    "                return {\"ocr_text\": \"\"}\n",
    "                \n",
    "            # 提取文字\n",
    "            text = []\n",
    "            for line in result:\n",
    "                if line:  # 確保 line 不為 None\n",
    "                    for word_info in line:\n",
    "                        if word_info and len(word_info) >= 2:\n",
    "                            text.append(word_info[1][0])\n",
    "                            \n",
    "            return {\n",
    "                \"ocr_text\": \" \".join(filter(None, text))\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"圖片處理錯誤: {str(e)}\")\n",
    "        return {\"ocr_text\": \"\"}\n",
    "        \n",
    "    finally:\n",
    "        # 清理臨時文件\n",
    "        if 'tmp_path' in locals():\n",
    "            try:\n",
    "                tmp_path.unlink()\n",
    "            except Exception as e:\n",
    "                logging.error(f\"刪除臨時文件錯誤: {str(e)}\")\n",
    "\n",
    "def read_pdf(pdf_loc, page_infos=None):\n",
    "    \"\"\"讀取 PDF 並處理其中的圖片\"\"\"\n",
    "    text_content = []\n",
    "    try:\n",
    "        doc = fitz.open(pdf_loc)\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc[page_num]\n",
    "            # 提取文字\n",
    "            text_content.append(page.get_text())\n",
    "            \n",
    "            # 提取並處理圖片\n",
    "            for img in page.get_images():\n",
    "                try:\n",
    "                    xref = img[0]\n",
    "                    base_image = doc.extract_image(xref)\n",
    "                    if base_image and \"image\" in base_image:\n",
    "                        image_results = process_image(base_image[\"image\"])\n",
    "                        if image_results[\"ocr_text\"]:\n",
    "                            text_content.append(image_results[\"ocr_text\"])\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"處理圖片錯誤 (頁碼 {page_num + 1}): {str(e)}\")\n",
    "                    continue\n",
    "                    \n",
    "        return ' '.join(filter(None, text_content))\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"PDF 處理錯誤: {str(e)}\")\n",
    "        return \"\"\n",
    "        \n",
    "    finally:\n",
    "        if 'doc' in locals():\n",
    "            doc.close()\n",
    "\n",
    "def read(pdf_loc, page_infos=None):\n",
    "    with pdfplumber.open(pdf_loc) as pdf:\n",
    "        pages = pdf.pages[page_infos[0]:page_infos[1]] if page_infos else pdf.pages\n",
    "        return ''.join(page.extract_text() for page in pages if page.extract_text())\n",
    "    \n",
    "x=read_pdf('./reference/finance/432.pdf')\n",
    "y=read('./reference/finance/432.pdf')\n",
    "print(x)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "def extract_pdf_content(pdf_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    同時提取 PDF 的文字和圖片\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    result = {\n",
    "        \"text\": [],\n",
    "        \"images\": []\n",
    "    }\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        \n",
    "        # 提取文字\n",
    "        text = page.get_text()\n",
    "        result[\"text\"].append(text)\n",
    "        \n",
    "        # 提取圖片\n",
    "        image_list = page.get_images()\n",
    "        for img_index, img in enumerate(image_list):\n",
    "            try:\n",
    "                # 取得圖片資料\n",
    "                xref = img[0]\n",
    "                base_image = doc.extract_image(xref)\n",
    "                image_bytes = base_image[\"image\"]\n",
    "                \n",
    "                # 轉換為 PIL Image 物件 (可選)\n",
    "                image = Image.open(io.BytesIO(image_bytes))\n",
    "                \n",
    "                # 儲存圖片資訊\n",
    "                result[\"images\"].append({\n",
    "                    \"page\": page_num,\n",
    "                    \"index\": img_index,\n",
    "                    \"image\": image,\n",
    "                    \"format\": base_image[\"ext\"],\n",
    "                    \"size\": image.size\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"處理圖片時發生錯誤: {str(e)}\")\n",
    "    \n",
    "    doc.close()\n",
    "    return result\n",
    "\n",
    "# 使用範例\n",
    "pdf_content = extract_pdf_content(\"./reference/finance/968.pdf\")\n",
    "\n",
    "# 存取文字\n",
    "all_text = \"\\n\".join(pdf_content[\"text\"])\n",
    "\n",
    "# 存取圖片\n",
    "for img_info in pdf_content[\"images\"]:\n",
    "    # 儲存圖片\n",
    "    img_info[\"image\"].save(f\"page_{img_info['page']}_img_{img_info['index']}.{img_info['format']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class QuestionOptimizer:\n",
    "    def __init__(self, model_name: str = \"Llama3-TAIDE-LX-8B-Chat-Alpha1\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        \n",
    "    def _generate_response(self, prompt: str) -> str:\n",
    "        \"\"\"使用模型生成回應\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,  # 明確啟用截斷\n",
    "            max_length=2048,  # 設定輸入長度限制\n",
    "            padding=True\n",
    "        ).to(self.model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=400,  # 只使用 max_new_tokens\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9\n",
    "            )\n",
    "            \n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def enhance_question(self, query: str, num_variants: int = 3) -> List[str]:\n",
    "        \"\"\"生成改寫後的問題\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "請依照以下規則，將問題改寫成{num_variants}個不同版本：\n",
    "1. 保持原始問題的核心意思\n",
    "2. 使用不同的表達方式\n",
    "3. 添加相關的關鍵詞\n",
    "4. 重組句子結構\n",
    "5. 確保改寫後的問題更容易被搜尋系統理解\n",
    "\n",
    "原始問題：{query}\n",
    "\"\"\"\n",
    "        response = self._generate_response(prompt)\n",
    "        enhanced_questions = [q.strip() for q in response.split('\\n') if q.strip()]\n",
    "        return enhanced_questions[:num_variants]\n",
    "\n",
    "    def process_batch(self, questions: List[dict]) -> List[dict]:\n",
    "        \"\"\"批次處理問題\"\"\"\n",
    "        return [{\n",
    "            'qid': q['qid'],\n",
    "            'original': q['query'],\n",
    "            'enhanced': self.enhance_question(q['query'])\n",
    "        } for q in questions]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    optimizer = QuestionOptimizer()\n",
    "    test_query = \"在2022年第3季，長榮公司有無從事衍生工具交易的情事？\"\n",
    "    enhanced = optimizer.enhance_question(test_query)\n",
    "    \n",
    "    print(\"原始問題:\", test_query)\n",
    "    print(\"\\n改寫後的問題:\")\n",
    "    for i, q in enumerate(enhanced, 1):\n",
    "        print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import *\n",
    "from llama_index.core.node_parser import SentenceSplitter,SentenceWindowNodeParser\n",
    "from llama_index.core import VectorStoreIndex, Document, StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# initialize the database\n",
    "db = VectorDatabase()\n",
    "dict = db.corpus_dict_finance\n",
    "\n",
    "my_embedding = HuggingFaceEmbedding(\n",
    "    model_name=\"TencentBAC/Conan-embedding-v1\"\n",
    ")\n",
    "# Prepare documents\n",
    "documents = [\n",
    "    Document(\n",
    "        text=text,\n",
    "        id_=f\"doc_id_{id}\",\n",
    "        metadata={\"category\": 'finance', \"pid\": id}\n",
    "    ) for id, text in dict.items()\n",
    "]\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=256, chunk_overlap=200)\n",
    "#\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")\n",
    "nodes_sentence = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "sentence_vector_index = VectorStoreIndex(\n",
    "    nodes=nodes_sentence,\n",
    "    embed_model=my_embedding,\n",
    "    show_progress=True,\n",
    ")\n",
    "sentence_vector_index.storage_context.persist(persist_dir='./database/sentence/fiance.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import *\n",
    "from llama_index.core.node_parser import SentenceSplitter,SentenceWindowNodeParser\n",
    "from llama_index.core import VectorStoreIndex, Document, StorageContext, load_index_from_storage\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# initialize the database\n",
    "db = VectorDatabase()\n",
    "dict = db.corpus_dict_finance\n",
    "\n",
    "my_embedding = HuggingFaceEmbedding(\n",
    "    model_name=\"TencentBAC/Conan-embedding-v1\"\n",
    ")\n",
    "# Prepare documents\n",
    "documents = [\n",
    "    Document(\n",
    "        text=text,\n",
    "        id_=f\"doc_id_{id}\",\n",
    "        metadata={\"category\": 'finance', \"pid\": id}\n",
    "    ) for id, text in dict.items()\n",
    "]\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=256, chunk_overlap=200)\n",
    "#\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")\n",
    "nodes_sentence = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "sentence_vector_index = VectorStoreIndex(\n",
    "    nodes=nodes_sentence,\n",
    "    embed_model=my_embedding,\n",
    "    show_progress=True,\n",
    ")\n",
    "sentence_vector_index.storage_context.persist(persist_dir='./database/sentence/fiance')\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, Document, StorageContext, load_index_from_storage\n",
    "index = load_index_from_storage(\n",
    "    StorageContext.from_defaults(persist_dir=\"./database/sentence/fiance\"),\n",
    "    embed_model=my_embedding,\n",
    ")\n",
    "\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.vector_stores import MetadataFilter, MetadataFilters, FilterOperator\n",
    "# Define metadata filters\n",
    "filters = MetadataFilters(\n",
    "    filters=[\n",
    "        MetadataFilter(key=\"category\", value=\"finance\", operator=FilterOperator.EQ),\n",
    "        MetadataFilter(key=\"pid\", value=[41, 70, 359, 870, 900, 951, 59], operator=FilterOperator.IN)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize the vector index retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=1,  # Retrieve all possible results\n",
    "    filters=filters\n",
    ")\n",
    "results = retriever.retrieve(\"鴻海在2023年第1季度中，集團存貨之帳面金額是多少？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.document_summary import DocumentSummaryIndexLLMRetriever\n",
    "\n",
    "from typing import Any, Callable, Dict, List, Optional, Union, cast\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from llama_index.core.llms.callbacks import llm_completion_callback\n",
    "from llama_index.core.llms import (\n",
    "    CustomLLM,\n",
    "    CompletionResponse,\n",
    "    CompletionResponseGen,\n",
    "    LLMMetadata,\n",
    ")\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model_name = \"Llama3-TAIDE-LX-8B-Chat-Alpha1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, low_cpu_mem_usage=True, device_map=\"cuda\", torch_dtype=torch.bfloat16).eval()\n",
    "#自定义本地模型\n",
    "class OurLLM(CustomLLM):\n",
    "    context_window: int = 4096\n",
    "    num_output: int = 1024\n",
    "    model_name_: str = \"custom\"\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        \"\"\"Get LLM metadata.\"\"\"\n",
    "        return LLMMetadata(\n",
    "            context_window=self.context_window,\n",
    "            num_output=self.num_output,\n",
    "            model_name_=self.model_name,\n",
    "        )\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
    "        text, history = model.chat(tokenizer, prompt, history=[], temperature=0.1)\n",
    "        return CompletionResponse(text=text)\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(\n",
    "            self, prompt: str, **kwargs: Any\n",
    "    ) -> CompletionResponseGen:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "system_prompt = \"你是一個來自台灣的AI助理，你的名字是 TAIDE，樂於以台灣人的立場幫助使用者，會用正體中文回答問題。\"\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "\"taide/Llama3-TAIDE-LX-8B-Chat-Alpha1\"\n",
    ")\n",
    "stopping_ids = [\n",
    "tokenizer.eos_token_id,\n",
    "tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]\n",
    "\n",
    "# # Transform a string into input zephyr-specific input\n",
    "# def completion_to_prompt(completion):\n",
    "#     return f\"<|system|>\\n</s>\\n<|user|>\\n{completion}</s>\\n<|assistant|>\\n\"\n",
    "\n",
    "\n",
    "# # Transform a list of chat messages into zephyr-specific input\n",
    "# def messages_to_prompt(messages):\n",
    "#     prompt = \"\"\n",
    "#     for message in messages:\n",
    "#         if message.role == \"system\":\n",
    "#             prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
    "#         elif message.role == \"user\":\n",
    "#             prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
    "#         elif message.role == \"assistant\":\n",
    "#             prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
    "\n",
    "#     # ensure we start with a system prompt, insert blank if needed\n",
    "#     if not prompt.startswith(\"<|system|>\\n\"):\n",
    "#         prompt = \"<|system|>\\n</s>\\n\" + prompt\n",
    "\n",
    "#     # add final assistant prompt\n",
    "#     prompt = prompt + \"<|assistant|>\\n\"\n",
    "\n",
    "#     return prompt\n",
    "\n",
    "\n",
    "import torch\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    model_name=\"taide/Llama3-TAIDE-LX-8B-Chat-Alpha1\",\n",
    "    tokenizer_name=\"taide/Llama3-TAIDE-LX-8B-Chat-Alpha1\",\n",
    "    context_window=2048,\n",
    "    max_new_tokens=400,\n",
    "    generate_kwargs={\"temperature\": 0.6, \"top_p\": 0.9, 'do_sample': True},\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    stopping_ids=stopping_ids,\n",
    "    tokenizer_kwargs={\"max_length\": 8000},\n",
    "    # messages_to_prompt=messages_to_prompt,\n",
    "    # completion_to_prompt=completion_to_prompt,\n",
    "    device_map=\"cuda\",\n",
    "    model_kwargs={\"torch_dtype\": torch.float16},\n",
    ")\n",
    "\n",
    "import logging\n",
    "\n",
    "# 定義 logging 輸出格式\n",
    "FORMAT = '%(asctime)s %(filename)s %(levelname)s:%(message)s'\n",
    "logging.basicConfig(level=logging.INFO, format=FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Any, Optional\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    " \n",
    "# Load \n",
    "path = \"reference/finance/\"\n",
    "file = \"351.pdf\"\n",
    "# Get elements\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=path+file,\n",
    "    extract_images_in_pdf=True,\n",
    "    infer_table_structure=True, \n",
    "    # Post processing to aggregate text once we have the title \n",
    "    chunking_strategy=\"by_title\",\n",
    "    # Chunking params to aggregate text blocks\n",
    "    # Require maximum chunk size of 4000 chars\n",
    "    # Attempt to create a new chunk at 3800 chars\n",
    "    # Attempt to keep chunks > 2000 chars \n",
    "    max_characters=4000, \n",
    "    new_after_n_chars=3800, \n",
    "    combine_text_under_n_chars=2000,\n",
    "    image_output_dir_path=path\n",
    ")\n",
    "\n",
    "class Element(BaseModel):\n",
    "    type: str\n",
    "    text: Any\n",
    " \n",
    "# Categorize by type\n",
    "categorized_elements = []\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
    "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
    " \n",
    "# Tables\n",
    "table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
    "print(table_elements) \n",
    "# output: 28 elements in the PDF file\n",
    " \n",
    "# Text\n",
    "text_elements = [e for e in categorized_elements if e.type == \"text\"]\n",
    "print(text_elements) \n",
    "# output: 127 elements in the PDF file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
